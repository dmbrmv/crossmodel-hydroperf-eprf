{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bc7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to Python path to access src modules\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "from src.interpolation import (\n",
    "    create_interpolated_datasets,\n",
    "    load_all_meteo_data,\n",
    ")\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefedb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load meteorological stations\n",
    "meteo_stations = gpd.read_file(\"../data/Geometry/meteo_stations.gpkg\")\n",
    "meteo_stations.set_index(\"gauge_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5171f",
   "metadata": {},
   "source": [
    "### Parse from initial .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd17b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = [\n",
    "    i.stem\n",
    "    for i in Path(\"../data/MeteoData/meteo_ru_2007_2022/\").glob(\"*.txt\")\n",
    "    if i.stem not in [\"statlist332041\", \"fld332041\"]\n",
    "]\n",
    "# Read the raw meteorological data file with proper column names\n",
    "column_names = [\n",
    "    \"station_id\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"t_min\",\n",
    "    \"col5\",\n",
    "    \"t_mean\",\n",
    "    \"col7\",\n",
    "    \"t_max\",\n",
    "    \"col9\",\n",
    "    \"prcp\",\n",
    "]\n",
    "for station_id in station_ids:\n",
    "    try:\n",
    "        meteo_raw = pd.read_csv(\n",
    "            f\"../data/MeteoData/meteo_ru_2007_2022/{station_id}.txt\", sep=\";\", header=None\n",
    "        )\n",
    "        meteo_raw.rename(\n",
    "            columns=dict(zip(range(len(column_names)), column_names, strict=False)), inplace=True\n",
    "        )\n",
    "        meteo_raw[\"date\"] = pd.to_datetime(\n",
    "            meteo_raw[[\"year\", \"month\", \"day\"]].astype(str).agg(\"-\".join, axis=1), format=\"%Y-%m-%d\"\n",
    "        )\n",
    "        meteo_raw.set_index(\"date\", inplace=True)\n",
    "        meteo_raw.drop(\n",
    "            columns=[\"station_id\", \"year\", \"month\", \"day\", \"col5\", \"col7\", \"col9\", 11, 12], inplace=True\n",
    "        )\n",
    "        meteo_raw[[\"t_min\", \"t_mean\", \"t_max\", \"prcp\"]] = meteo_raw[\n",
    "            [\"t_min\", \"t_mean\", \"t_max\", \"prcp\"]\n",
    "        ].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        meteo_raw.to_csv(f\"../data/MeteoData/meteo_ru/{station_id}.csv\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Empty data for station {station_id}, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e70c94",
   "metadata": {},
   "source": [
    "### Based on station geometry and data create .nc grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e9dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.interpolation.data_loader:Missing required columns for station 20289\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 21611\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 21647\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 23146\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 23331\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 23365\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 24105\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 25372\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 25621\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 25656\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 25777\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 26038\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 26188\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 26231\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 26422\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 26629\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 30253\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 31459\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 32564\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 34747\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 35133\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 35663\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 36974\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 37050\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 37385\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38353\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38507\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38599\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38750\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38836\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38880\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38933\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38974\n",
      "WARNING:src.interpolation.data_loader:Missing required columns for station 38987\n"
     ]
    }
   ],
   "source": [
    "# Load all meteorological data using new module\n",
    "data_dir = Path(\"../data/MeteoData/meteo_ru\")\n",
    "logger = setup_logger(name=\"MeteoGridder\", log_file=\"../logs/meteo_grider.log\")\n",
    "meteo_data_dict = load_all_meteo_data(meteo_stations, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f65f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2007-01-01 00:00:00 to 2024-10-31 00:00:00\n",
      "Total dates with data: 6514\n"
     ]
    }
   ],
   "source": [
    "# Define extent and parameters\n",
    "extent = (20.0, 50.0, 40.0, 70.0)  # West, East, South, North\n",
    "resolution = 0.2  # degrees\n",
    "\n",
    "# Get common date range from all stations\n",
    "all_dates = set()\n",
    "for df in meteo_data_dict.values():\n",
    "    all_dates.update(df.index)\n",
    "\n",
    "# Convert to sorted list and create date range\n",
    "common_dates = sorted(list(all_dates))\n",
    "if common_dates:\n",
    "    date_range = pd.date_range(start=min(common_dates), end=max(common_dates), freq=\"D\")\n",
    "    # Filter to dates that have data\n",
    "    date_range = pd.DatetimeIndex([d for d in date_range if d in all_dates])\n",
    "\n",
    "    print(f\"Date range: {date_range[0]} to {date_range[-1]}\")\n",
    "    print(f\"Total dates with data: {len(date_range)}\")\n",
    "else:\n",
    "    print(\"No common dates found in meteorological data\")\n",
    "    date_range = pd.DatetimeIndex([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecb97f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.interpolation.gridding:Starting interpolation for 6514 dates using cubic method\n",
      "INFO:src.interpolation.gridding:Processing date 1/6514: 2007-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interpolated datasets for 6514 dates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.interpolation.gridding:Processing date 101/6514: 2007-04-11 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 201/6514: 2007-07-20 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 301/6514: 2007-10-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 401/6514: 2008-02-05 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 501/6514: 2008-05-15 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 601/6514: 2008-08-23 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 701/6514: 2008-12-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 801/6514: 2009-03-11 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 901/6514: 2009-06-19 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1001/6514: 2009-09-27 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1101/6514: 2010-01-05 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1201/6514: 2010-04-15 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1301/6514: 2010-07-24 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1401/6514: 2010-11-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1501/6514: 2011-02-09 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1601/6514: 2011-05-20 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1701/6514: 2011-08-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1801/6514: 2011-12-06 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 1901/6514: 2012-03-15 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2001/6514: 2012-06-23 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2101/6514: 2012-10-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2201/6514: 2013-01-09 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2301/6514: 2013-04-19 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2401/6514: 2013-07-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2501/6514: 2013-11-05 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2601/6514: 2014-02-13 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2701/6514: 2014-05-24 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2801/6514: 2014-09-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 2901/6514: 2014-12-10 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3001/6514: 2015-03-20 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3101/6514: 2015-06-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3201/6514: 2015-10-06 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3301/6514: 2016-01-14 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3401/6514: 2016-04-23 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3501/6514: 2016-08-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3601/6514: 2016-11-09 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3701/6514: 2017-02-17 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3801/6514: 2017-05-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 3901/6514: 2017-09-05 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4001/6514: 2017-12-14 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4101/6514: 2018-03-24 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4201/6514: 2018-07-02 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4301/6514: 2018-10-10 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4401/6514: 2019-01-18 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4501/6514: 2019-04-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4601/6514: 2019-08-06 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4701/6514: 2019-11-14 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4801/6514: 2020-02-22 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 4901/6514: 2020-06-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5001/6514: 2020-09-09 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5101/6514: 2020-12-18 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5201/6514: 2021-03-28 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5301/6514: 2021-07-06 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5401/6514: 2021-10-14 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5501/6514: 2022-01-22 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5601/6514: 2022-05-02 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5701/6514: 2022-08-10 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5801/6514: 2022-11-18 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 5901/6514: 2023-02-26 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6001/6514: 2023-06-06 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6101/6514: 2023-09-14 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6201/6514: 2023-12-23 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6301/6514: 2024-04-01 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6401/6514: 2024-07-10 00:00:00\n",
      "INFO:src.interpolation.gridding:Processing date 6501/6514: 2024-10-18 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature dataset:\n",
      "<xarray.Dataset> Size: 4GB\n",
      "Dimensions:  (time: 6514, lat: 151, lon: 151)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 52kB 2007-01-01 2007-01-02 ... 2024-10-31\n",
      "  * lat      (lat) float64 1kB 40.0 40.2 40.4 40.6 40.8 ... 69.4 69.6 69.8 70.0\n",
      "  * lon      (lon) float64 1kB 20.0 20.2 20.4 20.6 20.8 ... 49.4 49.6 49.8 50.0\n",
      "Data variables:\n",
      "    t_min    (time, lat, lon) float64 1GB nan nan nan ... -1.076 -1.008 -0.9418\n",
      "    t_mean   (time, lat, lon) float64 1GB nan nan nan ... 0.09253 0.1231 0.1545\n",
      "    t_max    (time, lat, lon) float64 1GB nan nan nan ... 0.8735 0.9231 0.9773\n",
      "Attributes:\n",
      "    title:                 Interpolated Temperature Data\n",
      "    extent:                West=20.0, East=50.0, South=40.0, North=70.0\n",
      "    resolution:            0.2 degrees\n",
      "    source:                Meteorological station data\n",
      "    interpolation_method:  cubic\n",
      "\n",
      "Precipitation dataset:\n",
      "<xarray.Dataset> Size: 1GB\n",
      "Dimensions:  (time: 6514, lat: 151, lon: 151)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 52kB 2007-01-01 2007-01-02 ... 2024-10-31\n",
      "  * lat      (lat) float64 1kB 40.0 40.2 40.4 40.6 40.8 ... 69.4 69.6 69.8 70.0\n",
      "  * lon      (lon) float64 1kB 20.0 20.2 20.4 20.6 20.8 ... 49.4 49.6 49.8 50.0\n",
      "Data variables:\n",
      "    prcp     (time, lat, lon) float64 1GB nan nan nan ... -0.751 -0.6514 -0.5541\n",
      "Attributes:\n",
      "    title:                 Interpolated Precipitation Data\n",
      "    extent:                West=20.0, East=50.0, South=40.0, North=70.0\n",
      "    resolution:            0.2 degrees\n",
      "    source:                Meteorological station data\n",
      "    interpolation_method:  cubic\n",
      "\n",
      "Datasets saved to:\n",
      "Temperature: ../data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_temperature_cubic.nc\n",
      "Precipitation: ../data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_precipitation_cubic.nc\n"
     ]
    }
   ],
   "source": [
    "# Create interpolated datasets with extrapolation option\n",
    "if len(date_range) > 0:\n",
    "    print(f\"Creating interpolated datasets for {len(date_range)} dates...\")\n",
    "\n",
    "    # Option to test with border extrapolation\n",
    "    use_extrapolation = True\n",
    "    method = \"cubic\"\n",
    "\n",
    "    stations = meteo_stations\n",
    "    data_dict = meteo_data_dict\n",
    "\n",
    "    temp_dataset, prcp_dataset = create_interpolated_datasets(\n",
    "        meteo_stations=stations,\n",
    "        meteo_data_dict=data_dict,\n",
    "        extent=extent,\n",
    "        date_range=date_range,\n",
    "        resolution=resolution,\n",
    "        method=method,\n",
    "    )\n",
    "\n",
    "    print(\"Temperature dataset:\")\n",
    "    print(temp_dataset)\n",
    "    print(\"\\nPrecipitation dataset:\")\n",
    "    print(prcp_dataset)\n",
    "\n",
    "    # Save datasets\n",
    "    Path(\"../data/MeteoData/parsed_meteo/meteo_ru_single\").mkdir(parents=True, exist_ok=True)\n",
    "    temp_output_path = (\n",
    "        f\"../data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_temperature_{method}.nc\"\n",
    "    )\n",
    "    prcp_output_path = (\n",
    "        f\"../data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_precipitation_{method}.nc\"\n",
    "    )\n",
    "\n",
    "    # Clean time coordinate attributes before saving\n",
    "    temp_dataset_clean = temp_dataset.copy()\n",
    "    prcp_dataset_clean = prcp_dataset.copy()\n",
    "\n",
    "    if \"units\" in temp_dataset_clean.time.attrs:\n",
    "        del temp_dataset_clean.time.attrs[\"units\"]\n",
    "    if \"units\" in prcp_dataset_clean.time.attrs:\n",
    "        del prcp_dataset_clean.time.attrs[\"units\"]\n",
    "\n",
    "    temp_dataset_clean.to_netcdf(temp_output_path)\n",
    "    prcp_dataset_clean.to_netcdf(prcp_output_path)\n",
    "\n",
    "    print(\"\\nDatasets saved to:\")\n",
    "    print(f\"Temperature: {temp_output_path}\")\n",
    "    print(f\"Precipitation: {prcp_output_path}\")\n",
    "else:\n",
    "    print(\"No data available for interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0effe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a986bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(\n",
    "    \"/home/dmbrmv/Development/MeteoSources/data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_precipitation_cubic.nc\"\n",
    ") as prcp_file:\n",
    "    pass\n",
    "\n",
    "with xr.open_dataset(\n",
    "    \"/home/dmbrmv/Development/MeteoSources/data/MeteoData/parsed_meteo/meteo_ru_single/interpolated_temperature_cubic.nc\"\n",
    ") as temp_file:\n",
    "    pass\n",
    "\n",
    "\n",
    "meteo_nc = Path(\"../data/MeteoData/parsed_meteo/meteo_ru_nc_02\")\n",
    "temp_path = meteo_nc / \"2m_temperature\"\n",
    "prcp_path = meteo_nc / \"total_precipitation\"\n",
    "temp_path.mkdir(parents=True, exist_ok=True)\n",
    "prcp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for year in range(2007, 2023):\n",
    "    temp_file.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "    prcp_file.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "    temp_file.to_netcdf(temp_path / f\"{year}.nc\")\n",
    "    prcp_file.to_netcdf(prcp_path / f\"{year}.nc\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
